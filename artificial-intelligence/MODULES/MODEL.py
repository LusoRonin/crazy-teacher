import numpy as np

# IMPLEMENTS THE MODEL CLASS DESIGNED TO TRAIN AND TEST A GIVEN CLASSIFIER AND EVALUATE IT.
class Model:
    # INITIALIZE THE MODEL WITH A GIVEN CLASSIFIER
    def __init__(self, CLASSIFIER, VALIDATION_SET_X, VALIDATION_SET_Y):
        self.CLASSIFIER = CLASSIFIER # CLASSIFIER
        self.CLASSIFIER_NAME = CLASSIFIER.__class__.__name__ # CLASSIFIER NAME
        self.VALIDATION_SET_X = VALIDATION_SET_X # VALIDATION SET CONTAINING THE SAMPLES: X
        self.VALIDATION_SET_Y = VALIDATION_SET_Y # VALIDATION SET CONTAINING THE CORRESPONDING LABELS: Y
        self.C_RANGE = [ 1, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9 ] # C RANGE
        self.T_RANGE = [ 1, 10, 100 ] # MAXIMUM ITERATIONS RANGE
        self.BEST_C = 0 # BEST C (IF THE CLASSIFIER IS BAYES; OTHERWISE, IT IS 0)
        self.BEST_T = 0 # BEST T: MAX ITERATIONS (IF THE CLASSIFIER IS BAYES; OTHERWISE, IT IS 0)

    # TRAIN(): TRAINS THE MODEL; X REPRESENTS A VECTOR OF SAMPLES, Y REPRESENTS A VECTOR OF CORRESPONDING LABELS
    def TRAIN(self, X, Y):
        self.CLASSIFIER.TRAIN(X, Y)

    # FIND_BEST_HYPERPARAMETERS(): FIND THE BEST HYPERPARAMETERS (C, T) BY TESTING ALL POSSIBLE COMBINATIONS OF HYPERPARAMETERS: C (IF THE CLASSIFIER IS BAYES), T (IF THE CLASSIFIER IS PERCEPTRON) AND SETS THEM. (X REPRESENTS A VECTOR OF TRAINING SAMPLES, Y REPRESENTS A VECTOR OF CORRESPONDING TRAINING LABELS)
    def FIND_BEST_HYPERPARAMETERS(self, X, Y):
        if self.CLASSIFIER_NAME == "Bayes": # IF THE CLASSIFIER IS BAYES
            BEST_ACCURACY = 0 # BEST ACCURACY
            for C in self.C_RANGE: # FOR EACH C IN THE C RANGE
                self.CLASSIFIER.C = C # SET THE C
                self.TRAIN(X, Y) # TRAIN THE MODEL
                ACCURACY = self.SCORE()[0] # CALCULATE THE ACCURACY
                if ACCURACY > BEST_ACCURACY: # IF THE ACCURACY IS BETTER THAN THE BEST ACCURACY
                    BEST_ACCURACY = ACCURACY # UPDATE THE BEST ACCURACY
                    self.BEST_C = C # UPDATE THE BEST C
            self.CLASSIFIER.C = self.BEST_C # SET THE BEST C
        elif self.CLASSIFIER_NAME == "Perceptron": # IF THE CLASSIFIER IS PERCEPTRON
            BEST_ACCURACY = 0 # BEST ACCURACY
            for T in self.T_RANGE: # FOR EACH T IN THE T RANGE
                self.CLASSIFIER.T = T # SET THE T
                self.TRAIN(X, Y) # TRAIN THE MODEL
                ACCURACY = self.SCORE()[0] # CALCULATE THE ACCURACY
                if ACCURACY > BEST_ACCURACY: # IF THE ACCURACY IS BETTER THAN THE BEST ACCURACY
                    BEST_ACCURACY = ACCURACY # UPDATE THE BEST ACCURACY
                    self.BEST_T = T # UPDATE THE BEST T
            self.CLASSIFIER.T = self.BEST_T # SET THE BEST T

    # CLASSIFY(): PREDICTS THE OUTPUT OF A SINGLE SAMPLE (X); RETURNS 1 IF SPAM, 0 IF HAM
    def CLASSIFY(self, X):
        return self.CLASSIFIER.CLASSIFY(X)

    # CALCULATE THE ACCURACY (MEASURES THE RATIO OF CORRECT PREDICTIONS OVER THE TOTAL NUMBER OF INSTANCES EVALUATED), ERROR RATE (MISCLASSIFICATION ERROR MEASURES THE RATIO OF INCORRECT PREDICTIONS OVER THE TOTAL NUMBER OF INSTANCES EVALUATED), SENSITIVITY (MEASURES THE FRACTION OF POSITIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED), SPECIFICITY (MEASURES THE FRACTION OF NEGATIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED), PRECISION (MEASURES THE POSITIVE PATTERNS THAT ARE CORRECTLY PREDICTED FROM THE TOTAL PREDICTED PATTERNS IN A POSITIVE CLASS), RECALL (MEASURES THE FRACTION OF POSITIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED), F-MEASURE (REPRESENTS THE HARMONIC MEAN BETWEEN RECALL AND PRECISION VALUES), GEOMETRIC MEAN (THIS METRIC IS USED TO MAXIMIZE THE TRUE POSITIVES RATE AND TRUE NEGATIVES RATE, AND SIMULTANEOUSLY KEEPING BOTH RATES RELATIVELY BALANCED), AND CONFUSION MATRIX (A TABLE THAT IS USED TO DESCRIBE THE PERFORMANCE OF A CLASSIFICATION MODEL ON A SET OF TEST DATA FOR WHICH THE TRUE VALUES ARE KNOWN)
    def SCORE(self, X = None, Y = None):
        X = self.VALIDATION_SET_X if X is None else X # IF X IS NONE, USE THE VALIDATION SET X
        Y = self.VALIDATION_SET_Y if Y is None else Y # IF Y IS NONE, USE THE VALIDATION SET Y
        PREDICTIONS = [self.CLASSIFY(_X) for _X in X] # PREDICT THE OUTPUT OF THE TEST DATA
        TRUE_POSITIVES = [ 1 if PREDICTIONS[I] == 1 and Y[I] == 1 else 0 for I in range(len(Y)) ].count(1) # CALCULATE THE TRUE POSITIVES
        FALSE_POSITIVES = [ 1 if PREDICTIONS[I] == 1 and Y[I] == -1 else 0 for I in range(len(Y)) ].count(1) # CALCULATE THE FALSE POSITIVES
        FALSE_NEGATIVES = [ 1 if PREDICTIONS[I] == -1 and Y[I] == 1 else 0 for I in range(len(Y)) ].count(1) # CALCULATE THE FALSE NEGATIVES
        TRUE_NEGATIVES = [ 1 if PREDICTIONS[I] == -1 and Y[I] == -1 else 0 for I in range(len(Y)) ].count(1) # CALCULATE THE TRUE NEGATIVES
        ACCURACY = (TRUE_POSITIVES + TRUE_NEGATIVES) / len(Y) # CALCULATE THE ACCURACY OF THE PREDICTIONS (MEASURES THE RATIO OF CORRECT PREDICTIONS OVER THE TOTAL NUMBER OF INSTANCES EVALUATED)
        ERROR_RATE = (FALSE_POSITIVES + FALSE_NEGATIVES) / len(Y) # CALCULATE THE ERROR RATE OF THE PREDICTIONS (MEASURES THE RATIO OF INCORRECT PREDICTIONS OVER THE TOTAL NUMBER OF INSTANCES EVALUATED)
        SENSITIVITY = TRUE_POSITIVES / (TRUE_POSITIVES + FALSE_NEGATIVES) # CALCULATE THE SENSITIVITY OF THE PREDICTIONS (MEASURES THE FRACTION OF POSITIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED: TRUE POSITIVES / (TRUE POSITIVES + FALSE NEGATIVES))
        SPECIFICITY = TRUE_NEGATIVES / (TRUE_NEGATIVES + FALSE_POSITIVES) # CALCULATE THE SPECIFICITY OF THE PREDICTIONS (MEASURES THE FRACTION OF NEGATIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED: TRUE NEGATIVES / (TRUE NEGATIVES + FALSE POSITIVES))
        PRECISION = TRUE_POSITIVES / (TRUE_POSITIVES + FALSE_POSITIVES) # CALCULATE THE PRECISION OF THE PREDICTIONS (MEASURES THE POSITIVE PATTERNS THAT ARE CORRECTLY PREDICTED FROM THE TOTAL PREDICTED PATTERNS IN A POSITIVE CLASS: TRUE POSITIVES / (TRUE POSITIVES + FALSE POSITIVES))
        RECALL = TRUE_POSITIVES / (TRUE_POSITIVES + TRUE_NEGATIVES) # CALCULATE THE RECALL OF THE PREDICTIONS (MEASURES THE FRACTION OF POSITIVE PATTERNS THAT ARE CORRECTLY CLASSIFIED: TRUE POSITIVES / (TRUE POSITIVES + FALSE NEGATIVES))
        F_MEASURE = 2 * (PRECISION * RECALL) / (PRECISION + RECALL) # CALCULATE THE F-MEASURE OF THE PREDICTIONS (REPRESENTS THE HARMONIC MEAN BETWEEN RECALL AND PRECISION VALUES: 2 * (PRECISION * RECALL) / (PRECISION + RECALL))
        GEOMETRIC_MEAN = np.sqrt(TRUE_POSITIVES * TRUE_NEGATIVES) # CALCULATE THE GEOMETRIC MEAN OF THE PREDICTIONS (THIS METRIC IS USED TO MAXIMIZE THE TRUE POSITIVES RATE AND TRUE NEGATIVES RATE, AND SIMULTANEOUSLY KEEPING BOTH RATES RELATIVELY BALANCED)
        CONFUSION_MATRIX = np.array([
            [TRUE_POSITIVES, FALSE_POSITIVES],
            [FALSE_NEGATIVES, TRUE_NEGATIVES]
        ]) # CREATE A CONFUSION MATRIX
        return ACCURACY, ERROR_RATE, SENSITIVITY, SPECIFICITY, PRECISION, RECALL, F_MEASURE, GEOMETRIC_MEAN, CONFUSION_MATRIX